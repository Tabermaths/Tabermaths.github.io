<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://taber.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://taber.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-29T19:10:45+00:00</updated><id>https://taber.github.io/feed.xml</id><title type="html">blank</title><subtitle>Dual PhD Candidate Statistics &amp; AI (Heriot-Watt University &amp; Université de Paris Cité). </subtitle><entry><title type="html">Self-supervised learning for imaging tutorial</title><link href="https://taber.github.io/blog/selfsuptutorial/" rel="alternate" type="text/html" title="Self-supervised learning for imaging tutorial"/><published>2024-08-27T19:22:00+00:00</published><updated>2024-08-27T19:22:00+00:00</updated><id>https://taber.github.io/blog/selfsuptutorial</id><content type="html" xml:base="https://taber.github.io/blog/selfsuptutorial/"><![CDATA[<p>This page contains information about the tutorial on self-supervised learning for imaging, given by <a href="https://www.eng.ed.ac.uk/about/people/professor-michael-e-davies">Mike Davies</a> and I. The tutorial is part of the <a href="https://eusipcolyon.sciencesconf.org/resource/page/id/28">EUSIPCO 2024 conference</a> on the 26/08/2024 in Lyon, France, and the MAC-MIGS doctoral school given at the University of Edinburgh in February 2025.</p> <p><a href="https://youtube.com/playlist?list=PLrflIVF5S9hDfFKdH3yAgNrLnP2JF6WBN&amp;si=t46JCApNlTSsuNDz">YouTube Playlist</a></p> <p><strong>Videos &amp; Slides</strong>:</p> <ol> <li>Introduction. <a href="https://youtu.be/gf-WCHXAdfk">video</a>, <a href="/assets/pdf/part1.pdf">slides</a></li> <li>Learning from noisy data <a href="https://youtu.be/dxgvrooTZqQ">video</a>, <a href="/assets/pdf/part2.pdf">slides</a></li> <li>Learning from incomplete operators. <a href="https://youtu.be/bIjEmd0kGN8">video</a>, <a href="/assets/pdf/part3.pdf">slides</a></li> <li>Equivariant imaging. <a href="https://youtu.be/7M52ensBFxM">video</a>, <a href="/assets/pdf/part4.pdf">slides</a></li> <li>Identification theory. <a href="https://youtu.be/Z1N7o9PlRIc">video</a>, <a href="/assets/pdf/part5.pdf">slides</a></li> <li>Perspectives. <a href="https://youtu.be/pk6Sl52c9x4">video</a>, <a href="/assets/pdf/part6.pdf">slides</a></li> </ol> <p>Bonus videos:</p> <ul> <li><a href="https://youtu.be/p0KXiReF3sg?si=mw5rMkYFAkoYFr4Q">UNSURE: Unknown Noise level Stein’s Unbiased Risk Estimate</a></li> <li><a href="https://youtu.be/C9IHJm_Ie2k?si=FBOzZ9tDu63k_Hdz">Generalized Recorrupted2Recorrupted</a></li> <li><a href="https://youtu.be/3JtrPP7qUCY?si=a3304Ad6viMwF_e7">Equivariant Imaging with real-world group transforms</a></li> </ul> <p><strong>Code</strong>: We will follow the Google Colab demo in <a href="https://colab.research.google.com/drive/1_dlXdNbgwg5u7_OAl29WiMRMOybnkCIo?usp=sharing">this link</a>. Advanced self-supervised learning <a href="https://andrewwango.github.io/deepinv-selfsup-fastmri">MRI benchmarking code</a> by Andrew Wang. More self-supervised learning demos can be found on the deepinverse website <a href="https://deepinv.github.io/deepinv/auto_examples/index.html#self-supervised-learning">here</a>.</p> <p><strong>Abstract</strong>: This tutorial will cover core concepts and recent advances in the emerging field of self-supervised learning methods for solving imaging inverse problems with deep neural networks. Self-supervised learning is a fundamental tool deploying deep learning solutions in scientific and medical imaging applications where obtaining a large dataset of ground-truth images is very expensive or impossible. The tutorial will provide a comprehensive summary of different self-supervised methods, discuss their theoretical underpinnings and present practical self-supervised imaging applications.</p> <h3 id="references">References</h3> <p>List of references mentioned in the tutorial by topic.</p> <h4 id="part-i-introduction">Part I: Introduction</h4> <ul> <li>Zbontar, Jure, et al. “fastMRI: An open dataset and benchmarks for accelerated MRI.” arXiv preprint arXiv:1811.08839 (2018).</li> <li>Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. “Deep image prior.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.</li> <li>Jin K. H., McCann M. T., Froustey E., Unser M., Deep convolutional neural network for inverse problems in imaging. IEEE Trans. Im. Proc., 2017.</li> <li>Monga V., Li Y., Eldar Y. C., Algorithm unrolling: interpretable, efficient deep learning for signal and image processing. IEEE Sig. Proc. Mag., 2021.</li> <li>Y. Zhu et al., “Denoising Diffusion Models for Plug-and-Play Image Restoration,” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Vancouver, BC, Canada, 2023, pp. 1219-1229.</li> </ul> <h4 id="part-ii-learning-from-noisy-data">Part II: Learning from noisy data</h4> <h5 id="noise2noise-methods">Noise2Noise methods</h5> <ul> <li>Mallows, Colin L. “Some comments on Cp.” Technometrics 15.4 (1973): 661-675.</li> <li>Lehtinen, Jaakko, et al. “Noise2noise: Learning image restoration without clean data.” Proceedings of the 35th International Conference on Machine Learning. 2018.</li> </ul> <h5 id="sure-methods">SURE methods</h5> <ul> <li>Stein, Charles M. “Estimation of the mean of a multivariate normal distribution.” The annals of Statistics (1981): 1135-1151.</li> <li>Breiman, Leo. “The little bootstrap and other methods for dimensionality selection in regression: X-fixed prediction error.” Journal of the American Statistical Association 87.419 (1992): 738-754.</li> <li>Hudson, H. Malcolm. “A natural identity for exponential families with applications in multiparameter estimation.” The Annals of Statistics 6.3 (1978): 473-484.</li> <li>Ramani, Sathish, Thierry Blu, and Michael Unser. “Monte-Carlo SURE: A black-box optimization of regularization parameters for general denoising algorithms.” IEEE Transactions on image processing 17.9 (2008): 1540-1554.</li> <li>Eldar, Yonina C. “Generalized SURE for exponential families: Applications to regularization.” IEEE Transactions on Signal Processing 57.2 (2008): 471-481.</li> <li>Metzler, Christopher A., et al. “Unsupervised learning with Stein’s unbiased risk estimator.” arXiv preprint arXiv:1805.10531 (2018).</li> <li>Kim, Kwanyoung, and Jong Chul Ye. “Noise2score: tweedies approach to self-supervised image denoising without clean images.” Advances in Neural Information Processing Systems 34 (2021): 864-874.</li> <li>Tachella, Julian, Mike Davies, and Laurent Jacques. “UNSURE: Unknown Noise level Stein’s Unbiased Risk Estimator.” ICLR (2024).</li> </ul> <h5 id="noisier2noise-methods">Noisier2Noise methods</h5> <ul> <li>Moran, Nick, et al. “Noisier2noise: Learning to denoise from unpaired noisy data.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</li> <li>Pang, Tongyao, et al. “Recorrupted-to-recorrupted: Unsupervised deep learning for image denoising.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.</li> <li>Oliveira, Natalia L., Jing Lei, and Ryan J. Tibshirani. “Unbiased risk estimation in the normal means problem via coupled bootstrap techniques.” arXiv preprint arXiv:2111.09447 (2021).</li> <li>Oliveira, Natalia L., Jing Lei, and Ryan J. Tibshirani. “Unbiased test error estimation in the poisson means problem via coupled bootstrap techniques.” arXiv preprint arXiv:2212.01943 (2022).</li> <li>Monroy, Brayan, Jorge Bacca, and Julian Tachella. “Generalized Recorrupted-to-Recorrupted: Self-Supervised Learning Beyond Gaussian Noise.” CVPR (2025).</li> </ul> <h5 id="noise2void-and-cross-validation-methods">Noise2Void and cross-validation methods</h5> <ul> <li>Efron, Bradley. “The estimation of prediction error: covariance penalties and cross-validation.” Journal of the American Statistical Association 99.467 (2004): 619-632.</li> <li>Krull, Alexander, Tim-Oliver Buchholz, and Florian Jug. “Noise2void-learning denoising from single noisy images.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.</li> <li>Batson, Joshua, and Loic Royer. “Noise2self: Blind denoising by self-supervision.” International Conference on Machine Learning. PMLR, 2019.</li> <li>Huang, Tao, et al. “Neighbor2neighbor: Self-supervised denoising from single noisy images.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.</li> <li>Hendriksen, Allard Adriaan, Daniel Maria Pelt, and K. Joost Batenburg. “Noise2inverse: Self-supervised deep convolutional denoising for tomography.” IEEE Transactions on Computational Imaging 6 (2020): 1320-1335.</li> </ul> <h5 id="blind-spot-networks">Blind spot networks</h5> <ul> <li>Laine, Samuli, et al. “High-quality self-supervised deep image denoising.” Advances in Neural Information Processing Systems 32 (2019).</li> <li>W Lee, S Son, K M Lee; AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 17725-17734</li> </ul> <h4 id="part-iii-learning-from-incomplete-operators">Part III: Learning from incomplete operators</h4> <ul> <li>Liu, Jiaming, et al. “RARE: Image reconstruction using deep priors learned without groundtruth.” IEEE Journal of Selected Topics in Signal Processing 14.6 (2020): 1088-1099.</li> <li>Tachella, Julian, Dongdong Chen, and Mike Davies. “Unsupervised learning from incomplete measurements for inverse problems.” Advances in Neural Information Processing Systems 35 (2022): 4983-4995.</li> <li>Yaman, Burhaneddin, et al. “Self supervised learning of physics guided reconstruction neural networks without fully sampled reference data.” Magnetic resonance in medicine 84.6 (2020): 3172-3191.</li> <li>Daras, Giannis, et al. “Ambient diffusion: Learning clean distributions from corrupted data.” Advances in Neural Information Processing Systems 36 (2024).</li> <li>Gan W. et al., Self-Supervised Deep Equilibrium Models with Theoretical Guarantees and Applications to MRI Reconstruction. IEEE Trans. Comp Imag., 2023.</li> <li>C. Millard and M. Chiew, “A Theoretical Framework for Self-Supervised MR Image Reconstruction Using Sub-Sampling via Variable Density Noisier2Noise,” in IEEE Transactions on Computational Imaging, vol. 9, pp. 707-720, 2023.</li> </ul> <h4 id="part-iv-equivariant-imaging">Part IV: Equivariant Imaging</h4> <ul> <li>Chen, Dongdong, Julian Tachella, and Mike E. Davies. “Equivariant imaging: Learning beyond the range space.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.</li> <li>Chen, Dongdong, Julian Tachella, and Mike E. Davies. “Robust equivariant imaging: a fully unsupervised framework for learning to image from noisy and partial measurements.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</li> <li>Scanvic, Jeremy, et al. “Self-supervised learning for image super-resolution and deblurring.” arXiv preprint arXiv:2312.11232 (2023).</li> <li>Wang, Andrew, and Mike Davies. “Perspective-equivariant imaging: an unsupervised framework for multispectral pansharpening.” ECCV Workshops (2024).</li> <li>Wang, Andrew, and Mike Davies. “Fully Unsupervised Dynamic MRI Reconstruction via Diffeo-Temporal Equivariance.” ISBI (2025).</li> </ul> <h4 id="part-v-identification-theory">Part V: Identification theory</h4> <ul> <li>Sauer, Tim, James A. Yorke, and Martin Casdagli. “Embedology.” Journal of statistical Physics 65 (1991): 579-616.</li> <li>Tachella, Julian, Dongdong Chen, and Mike Davies. “Sensing theorems for unsupervised learning in linear inverse problems.” Journal of Machine Learning Research 24.39 (2023): 1-45.</li> <li>Cramer, Harald; Wold, Herman (1936). “Some Theorems on Distribution Functions”. Journal of the London Mathematical Society. 11 (4): 290-294.</li> <li>Bourrier A., Davies M. E., Peleg T., Perez P., Gribonval R. Fundamental Performance Limits for Ideal Decoders in High-Dimensional Linear Inverse Problems. IEEE Trans. Inf. Thy., 2014.</li> </ul> <h4 id="part-vi-perspectives">Part VI: Perspectives</h4> <ul> <li>Bora, Ashish, Eric Price, and Alexandros G. Dimakis. “AmbientGAN: Generative models from lossy measurements.” International conference on learning representations. 2018.</li> <li>Hermosilla, Pedro, Tobias Ritschel, and Timo Ropinski. “Total denoising: Unsupervised learning of 3D point cloud cleaning.” Proceedings of the IEEE/CVF international conference on computer vision. 2019.</li> <li>Tachella, Julian, and Laurent Jacques. “Learning to reconstruct signals from binary measurements.” arXiv preprint arXiv:2303.08691 (2023).</li> <li>Bellec, Pierre C., and Cun-Hui Zhang. “Second-order Stein: SURE for SURE and other applications in high-dimensional inference.” The Annals of Statistics 49.4 (2021): 1864-1903.</li> <li>Tachella, Julian, and Marcelo Pereyra. “Equivariant bootstrapping for uncertainty quantification in imaging inverse problems.” AISTATS (2024).</li> </ul>]]></content><author><name></name></author><category term="self-supervised learning"/><summary type="html"><![CDATA[information about EUSIPCO'24 tutorial]]></summary></entry><entry><title type="html">Unsupervised Learning to Solve Inverse Problems</title><link href="https://taber.github.io/blog/equivariantimaging/" rel="alternate" type="text/html" title="Unsupervised Learning to Solve Inverse Problems"/><published>2022-05-10T19:22:00+00:00</published><updated>2022-05-10T19:22:00+00:00</updated><id>https://taber.github.io/blog/equivariantimaging</id><content type="html" xml:base="https://taber.github.io/blog/equivariantimaging/"><![CDATA[<p>Inverse problems are ubiquitous in signal and image processing. In most applications, we need to reconstruct an underlying signal \(x\in\mathbb{R}^{n}\), from some measurements \(y\in\mathbb{R}^{m}\), that is, invert the forward measurement process, \begin{equation} y = Ax+n \end{equation} where \(n\) represents some noise and \(A\) is the forward operator. Due to the ill-posed nature of \(A\) (we generally have \(m&lt;n\)) and noise, there are multiple possible solutions \(x\) for a given \(y\). Fortunately, the set of plausible (natural) signals \(x\) lie in a small low-dimensional set \(\mathcal{X}\) of the whole of \(\mathbb{R}^{n}\), so we can have a unique \(x\) for a given \(y\).</p> <p>The traditional approach is to build a mathematical model to describe \(\mathcal{X}\) leveraging some prior knowledge about the underlying signals (e.g. natural images can be described as piecewise smooth). However, this a hard task which is problem-dependent and it is generally a loose description of the true \(\mathcal{X}\).</p> <p>In recent years, an alternative approach is to learn inverse mapping from \(y\mapsto x\) directly from training data, bypassing the need to design a prior model. Fuelled by the powerful learning bias of deep convolutional neural networks (interest readers can have a look at my previous post about understanding this implicit bias), the goal is to learn a function \(x=f(y)\) from training pairs \((x_i,y_i)\). The fundamental limitation of this approach is that in many real world applications we can only access \(y\). Training only with the \(y_i\) (enforcing measurement consistency) accounts to finding an \(f\) such that \(y=A f(y)\). Unfortunately this is doomed to fail, as there are infinite possible functions \(f\) that can fit the measurements perfectly well! This is because any \(f\) can output any value in the nullspace of \(A\) and still achieve measurement consistency. In other words, this fundamental limitation is a chicken-and-egg problem: we cannot learn to solve an inverse problem without solving it first to obtain the ground-truth training data!</p> <p>In (missing reference), we show that this problem can be overcome by adding a small assumption to the underlying set of signals \(\mathcal{X}\): invariance. It is well-known that most natural signals posses some kind of invariance. For example, images are generally invariant to shifts or rotations. Hence, the whole sensing process \(x = (f \circ A) (x)\) is necessarily an equivariant function, that is, given a transformation \(T_g\) (e.g. a shift), we have that \begin{equation} T_gx = (f\circ A) (T_gx). \end{equation} The invariance gives us information of the nullspace of A, which boils down to the following observation: \begin{equation} y=Ax = AT_g x’ = A_g x’ \end{equation} which just relies on the fact that \(x'= T_gx\) is another valid signal. Hence we can see beyond the range space of \(A\), as we have an implicit access to multiple different operators \(A_g = AT_g\) for all possible transformations \(T_1,\dots,T_{G}\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ei_iccv-480.webp 480w,/assets/img/ei_iccv-800.webp 800w,/assets/img/ei_iccv-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ei_iccv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Learning to image from only measurements. Training an imaging network through just measurement consistency (MC) does not significantly improve the reconstruction over the simple pseudo-inverse. However, by enforcing invariance in the reconstructed image set, equivariant imaging (EI) performs almost as well as a fully supervised network. Top: sparse view CT reconstruction, Bottom: pixel inpainting. PSNR is shown in top right corner of the images </div> <p>(missing reference) shows that the invariance constraint on \((f\circ A)\) can be easily incorporated as an additional loss term when training a deep network. In (missing reference) we extended the unsupervised method to account for noise. The method builds an unsupervised loss using Stein’s unbiased risk (SURE) estimator, which approximates the noiseless measurement consistency.</p> <p>Experiments in (missing reference) and (missing reference) show that for the computed tomography and inpaiting problems, the equivariant learning approach (only having access to measurements \(y_i\)) performs as well as the fully supervised case i.e. having training pairs with ground-truth data \((x_i,y_i)\), by-passing the fundamental limitation of learning to solve inverse problems.</p> <h2 id="theory">Theory</h2> <p>Despite the good empirical results, a few important theoretical questions arise: <strong>When is unsupervised learning possible?</strong> How big has the group invariance has to be? How many measurements per observation do we need?</p> <p>We provide answers to these questions in (missing reference):</p> <h3 id="necessary-conditions">Necessary Conditions</h3> <p>In order to learn from measurement data alone, we need that the set range spaces of virtual operators span the full ambient space \(\mathbb{R}^{n}\), i.e.,</p> \[\begin{equation}\label{eq:necessary} \text{rank}\begin{bmatrix} AT_1 \\ \vdots \\ AT_G \end{bmatrix} = n \end{equation}\] <p>This condition requires that \(m \geq \max_j c_j/s_j\) where \({s_j}\) and \({c_j}\) are the dimension and multiplicities of the irreducible representations of the group action. Most group symmetries (translations, reflections or rotations of a signal) appearing in practice have \(\max_j c_j/s_j=n/G\). In this case, we need at least \begin{equation} m \geq n/G \end{equation} measurements.</p> <p>Moreover, condition \eqref{eq:necessary} requires that the forward operator \(A\) is <strong>not</strong> equivariant to the group action. Otherwise, the concatenation of \(AT_1,\dots, AT_{G}\) has rank \(m&lt;n\).</p> <h3 id="sufficient-condition">Sufficient Condition</h3> <p>In order to guarantee unique model recovery, we need to take into account the dimension of the signal set \(\mathcal{X}\). Let \(k\) be the box-counting dimension of \(\mathcal{X}\) and let \(G\) be a cyclic group where \(\{c_j\}\) denote th multiplicities of the irreducible representations. Then, almost every forward operator \(A\in \mathbb{R}^{m\times n}\) with \(m&gt; 2k + 1 + \max_j c_j\). Most cyclic group symmetries (translations, reflections or rotations of a signal) appearing in practice have \(\max_j c_j=n/G\). In these cases we have that fully self-supervised learning is possible by almost every \(A\) with \begin{equation} m&gt; 2k + 1 + n/G \end{equation} measurements.</p> <h4 id="multiple-operators">Multiple operators</h4> <p>If the signal set is not group invariant, but we observe measurements via different operators \(A_1,\dots,A_G\), then unsupervised from measurement data alone is possible. In this case the necessary condition on the number of measurements is \(m\geq n/G\), and the sufficient condition is \(m&gt;n/G+k\). These results are included in (missing reference).</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> </div>]]></content><author><name></name></author><category term="self-supervised learning"/><summary type="html"><![CDATA[How to learn from incomplete data only]]></summary></entry><entry><title type="html">Understanding the Deep Image Prior</title><link href="https://taber.github.io/blog/understandingcnns/" rel="alternate" type="text/html" title="Understanding the Deep Image Prior"/><published>2022-01-10T19:22:00+00:00</published><updated>2022-01-10T19:22:00+00:00</updated><id>https://taber.github.io/blog/understandingcnns</id><content type="html" xml:base="https://taber.github.io/blog/understandingcnns/"><![CDATA[<p>Convolutional neural networks (CNNs) are a well-established tool for solving computational imaging problems. It has been recently shown that, despite being highly overparameterized (more weights than pixels), networks trained with a single corrupted image can still perform as well as fully trained networks (a.k.a. the deep image prior). These results highlight that CNNs posses a very powerful learning bias towards natural images, which explains their great success in recent years. Multiple intriguing question arise:</p> <p><strong>What is the learning bias?</strong> Are neural networks performing something similar to other existing tools in signal processing? Is the existing theory able to explain this phenomenon?</p> <p>In (missing reference), we make a first step towards answering these questions, using recent theoretical insights of infinitely wide networks (a.k.a. the neural tangent kernel), elucidating formal links between CNNs and well-known non-local patch denoisers, such as non-local means.</p> <p>Non-local means uses the following non-local similarity function:</p> \[k(y_i, y_j) = \exp(-||y_i-y_j||^2/\sigma^2)\] <p>where \(y_i\) and \(y_j\) are small image patches (e.g. \(5\times 5\) pixels) around the pixels \(i\) and \(j\). The filter matrix \(W\) is constructed as \(W = \text{diag}(\frac{1}{1^TK}) K\) and the simplest denoising procedure consists of applying \(W\) to the (vectorized) noisy image \(y\), that is \(\hat{z}=W y\). There are more sophisticated procedures such as twicing, where the filtering matrix is applied iteratively to the residual:</p> \[z^{k+1} = z^{k} + W(y-z^{k})\] <p>This procedure trades bias (over-smooth estimates) for variance (noisy estimates), and is stopped when a good balance is achieved. How does this relate to a convolutional neural network trained with a single image? It turns out that, as the network’s width increases, standard gradient descent optimization of the squared \(\ell_2\) loss follows the twicing process, with a (fixed!) filter matrix \(W=K\) where the pixel affinity function is available in closed form and only depends on the architecture of the network! For example, a simple single-hidden layer network with a filter of \(k\times k\) pixels, corresponds to a non-local similarity function</p> \[k(y_i, y_j) = \frac{||y_i|| ||y_j||}{\pi} (\sin\phi+(\pi-\phi)\cos\phi)\] <p>where \(\phi\) is the angle between patches \(y_i\) and \(y_j\) of \(k\times k\) pixels each. Hence, we can compute the implicit filter in closed-form, without need to train a very large network!</p> <p>Our analysis reveals that a neural network that, while the NTK theory accurately predicts the filter associated with networks trained using standard gradient descent, it falls short to explain the behavior of networks trained using the popular Adam optimizer. The latter achieves a larger change of weights in hidden layers, adapting the non-local filtering function during training. We evaluate our findings via extensive image denoising experiments. See the paper for more details!</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> </div>]]></content><author><name></name></author><category term="deep-image-prior"/><summary type="html"><![CDATA[Trying to unveil the mystery]]></summary></entry><entry><title type="html">Sketching Single-Photon Data</title><link href="https://taber.github.io/blog/lidarsketching/" rel="alternate" type="text/html" title="Sketching Single-Photon Data"/><published>2021-10-11T19:22:00+00:00</published><updated>2021-10-11T19:22:00+00:00</updated><id>https://taber.github.io/blog/lidarsketching</id><content type="html" xml:base="https://taber.github.io/blog/lidarsketching/"><![CDATA[<p>Single-photon lidar is an emerging ranging technique that can obtain 3D information at kilometre distance with centimetre precision, and has important applications in self-driving cars, forest canopy monitoring, non-line-of-sight imaging and more. This modality consists of contracting a histogram of time-of-arrival of individual photons per pixel. For each object in the line-of-sight of the device there is a peak in the histogram. These peaks are found by a 3D reconstruction algorithm that takes into account the Poisson statistics of the photon-count data, while promoting spatial smoothness in the reconstructed point clouds. In a previous post, I presented an algorithm that can find multiple peaks per pixel in a matter of milliseconds even in challenging very long range scenarios with high background noise. As the algorithm needs to process the histogram data, the <strong>reconstruction time</strong> depends (linearly) on the total number of non-zero bins in the histogram:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/timing_bins-480.webp 480w,/assets/img/timing_bins-800.webp 800w,/assets/img/timing_bins-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/timing_bins.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Execution time of a 3D reconstruction algorithm as a function the number of non-zero bins in the collected time-of-arrival histograms </div> <p>As single-photon lidar arrays get bigger and faster, the number of photons collected per histogram gets bigger, while there is an increased need for faster real-time frame rates. The volume of photon data that needs to be transmitted is ever-increasingly large, generating a <strong>data transfer bottleneck</strong>. Moreover, reconstruction algorithms are required to deal with ever-increasingly large and dense histograms, generating a <strong>computational bottleneck</strong>. So far, most attempts to alleviate these bottlenecks consisted in building coarser histograms. Despite reducing the amount of information to be transferred and processed, this approach sacrifices important depth resolution.</p> <p>In (missing reference), we propose a sketching method to massively <strong>compress the histograms without any significant loss of information</strong>, removing the data and computational bottlenecks. The technique builds on recent advances in <a href="https://arxiv.org/abs/1706.07180">compressive learning</a>, a theory for compressing distributions. The compressed data consists of a series of \(K\) statistics</p> \[\Phi_k(t) = [\cos(w_k t), \sin(w_kt)]^{T} \quad \text{for} \quad k=1, \dots, K\] <p>where \(t\) denotes the time of arrival. The statistics can be <strong>computed on-the-fly</strong>, i.e. updated with each photon arrival, hence completely by-passing the need to construct a histogram. Below you can see the large difference between reducing the data by coarse binning the histogram and our proposed method:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sheehan2021sketching2-480.webp 480w,/assets/img/sheehan2021sketching2-800.webp 800w,/assets/img/sheehan2021sketching2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/sheehan2021sketching2.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Coarse binning and sketching </div> <p>In (missing reference), we propose detection methods (i.e., deciding whether there is a surface in a given pixel), which only require access to the sketched data and perform similarly to the other detection methods which require access to time-of-arrival histograms.</p> <p>In (missing reference), we introduce a framework for using sketching together with spatially regularised reconstruction method, which can be applied to most existing spatial reconstruction methods (for example the ones in <a href="/blog/3Dreconstruction/">this project</a>, and is able to massively reduce their computational complexity in mid and high photon regimes.</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> </div>]]></content><author><name></name></author><category term="lidar"/><summary type="html"><![CDATA[compressing large-scale lidar data]]></summary></entry><entry><title type="html">3D Reconstruction From Single-Photon Data</title><link href="https://taber.github.io/blog/3Dreconstruction/" rel="alternate" type="text/html" title="3D Reconstruction From Single-Photon Data"/><published>2020-02-10T00:00:00+00:00</published><updated>2020-02-10T00:00:00+00:00</updated><id>https://taber.github.io/blog/3Dreconstruction</id><content type="html" xml:base="https://taber.github.io/blog/3Dreconstruction/"><![CDATA[<p>Single-photon light detection and ranging (lidar) has emerged as a prime candidate technology for depth imaging through challenging environments. This modality relies on constructing, for each pixel, a histogram of time delays between emitted light pulses and detected photon arrivals. The problem of estimating the number of imaged surfaces, their reflectivity and position becomes very challenging in the low-photon regime (which equates to short acquisition times) or relatively high background levels (i.e., strong ambient illumination).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lidar_summary-480.webp 480w,/assets/img/lidar_summary-800.webp 800w,/assets/img/lidar_summary-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/lidar_summary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Schematic of 3D reconstruction from lidar data </div> <p>In a general setting, a variable number of surfaces can be observed per imaged pixel. The majority of existing methods assume exactly one surface per pixel, simplifying the reconstruction problem so that standard image processing techniques can be easily applied. However, this assumption hinders practical three-dimensional (3D) imaging applications, being restricted to controlled indoor scenarios. Moreover, other existing methods that relax this assumption achieve worse reconstructions, suffering from long execution times and large memory requirements.</p> <p>This project focuses on novel approaches to 3D reconstruction from single-photon lidar data, which are capable of identifying multiple surfaces in each pixel. A first approach to multi-depth consists of detecting in which pixels a target is present. Limiting the number of surfaces per pixel to 0 or 1 can significantly reduce the complexity of the reconstructions algorithms, while still tackling a wide range of practical imaging scenarios. Detection methods can be found in (missing reference) and (missing reference).</p> <p>The models proposed in (missing reference), (missing reference), (missing reference) and (missing reference) differ from standard image processing tools, being designed to capture correlations of manifold-like structures.</p> <p>Until now, a major limitation has been the significant amount of time required for the analysis of the recorded data. By combining statistical models with highly scalable computational tools from the computer graphics community, we demonstrate 3D reconstruction of complex outdoor scenes with processing times of the order of 20 ms, where the lidar data was acquired in broad daylight from distances up to 320 m (missing reference). This has enabled robust, real-time target reconstruction of complex moving scenes, paving the way for single-photon lidar at video rates for practical 3D imaging applications</p> <p>Multispectral lidar (MSL) systems gather measurements at many spectral bands, making it possible to distinguish distinct materials. The MSL modality consists of constructing one histogram of time delays per wavelength. 3D reconstruction from MSL data imposes an additional challenge as the data to be processed can become prohibitive. A way to overcome this limitation is through the use of compressive strategies on the spatial domain (missing reference).</p> <p>A comprehensive survey of 3D reconstruction methods can be found in (missing reference).</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> </div>]]></content><author><name></name></author><category term="lidar"/><summary type="html"><![CDATA[Single-photon light detection and ranging (lidar) has emerged as a prime candidate technology for depth imaging through challenging environments. This modality relies on constructing, for each pixel, a histogram of time delays between emitted light pulses and detected photon arrivals. The problem of estimating the number of imaged surfaces, their reflectivity and position becomes very challenging in the low-photon regime (which equates to short acquisition times) or relatively high background levels (i.e., strong ambient illumination).]]></summary></entry></feed>